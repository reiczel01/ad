{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mateusz Reczulski 236440 Analiza Danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Analiza absencji w miejscu pracy\n",
    "\n",
    "## Opis problemu\n",
    "Zbiór danych dotyczy absencji pracowników w miejscu pracy. Dane są zbierane z międzynarodowej firmy.\n",
    "Absencja pracowników może wpływać na produktywność firmy, a przez to na jej wyniki finansowe.\n",
    "Możliwe, że analiza tych danych pomoże zrozumieć, które czynniki najbardziej wpływają na absencję w pracy.\n",
    "\n",
    "## Źródło danych\n",
    "Zbiór danych, który będziemy analizować, pochodzi z UCI Machine Learning Repository\n",
    "i jest dostępny pod adresem: https://archive.ics.uci.edu/dataset/445/absenteeism+at+work\n",
    "\n",
    "Zawiera on informacje na temat absencji w pracy, takie jak powód absencji, miesiąc roku, dzień tygodnia,\n",
    "sezony, transport, odległość od miejsca zamieszkania do pracy, obciążenie pracą na dzień, czas spędzony\n",
    "w pracy, liczba dzieci, palacz czy niepalacz, spożycie alkoholu, liczba zwierząt domowych, wiek, stan zdrowia, nieobecność.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importowanie bibliotek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wczytanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = 'absenteeism+at+work/Absenteeism_at_work.csv'\n",
    "data = pd.read_csv(dir, sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opis danych\n",
    "\n",
    "Najpierw przyjrzymy się naszym danym, sprawdzając pierwsze pięć wierszy za pomocą metody `.head()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statystyki opisowe\n",
    "\n",
    "Metoda `.describe()` dostarcza statystyk opisowych dla wszystkich kolumn numerycznych w naszym DataFrame.\n",
    "Dla każdej kolumny otrzymamy następujące informacje:\n",
    "- `count` - ilość rekordów\n",
    "- `mean` - średnia wartość\n",
    "- `std` - odchylenie standardowe\n",
    "- `min` - minimalna wartość\n",
    "- `25%`, `50%`, `75%` - odpowiednio pierwszy kwartyl, mediana (drugi kwartyl) oraz trzeci kwartyl\n",
    "- `max` - maksymalna wartość\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wizualizacje danych\n",
    "\n",
    "Visualizacje danych są kluczowe do zrozumienia struktury naszych danych. Poniżej znajdują się dwa przykłady wizualizacji, które możemy wykorzystać.\n",
    "\n",
    "#### Mapa ciepła (Heatmap)\n",
    "\n",
    "Pierwsza to mapa ciepła korelacji, która pokazuje, jak nasze cechy są ze sobą powiązane. Na osiach mamy nazwy cech, a kolor każdej komórki reprezentuje siłę korelacji między cechami oznaczonymi na osiach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_data = data.drop('ID', axis=1).corr()\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_data, annot=True, cmap='coolwarm')\n",
    "plt.title(\"Mapa ciepła korelacji cech\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histogramy\n",
    "\n",
    "Histogramy są używane do wyświetlania rozkładu jednowymiarowego zestawu obserwacji. Dla każdej kolumny numerycznej w naszych danych, wygenerujemy histogram, który pokaże nam, jak często pojawiają się różne zakresy wartości."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('ID', axis=1).hist(bins=30, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przygotowanie danych\n",
    "\n",
    "### Czyszczenie danych\n",
    "\n",
    "Często w realnych zestawach danych pojawiają się brakujące wartości. Wykorzystamy metodę `.isnull().sum()` do sprawdzenia, czy w naszych danych są jakieś brakujące wartości."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprawdzanie brakujących danych\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W przypadku braku danych możemy zdecydować się na różne strategie, na przykład usunięcie tych rekordów,\n",
    "wypełnienie ich średnią lub medianą, bądź zastosowanie bardziej zaawansowanych metod takich jak\n",
    "imputacja wielokrotna.\n",
    "\n",
    "W tym przypadku, zakładamy że nie ma brakujących danych. Jeśli jednak takowe wystąpiły,\n",
    "poniżej znajduje się przykładowy kod, który wypełnia brakujące wartości medianą."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wypełnianie brakujących danych medianą\n",
    "# data.fillna(data.median(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wybór cech\n",
    "\n",
    "Następnie dokonamy selekcji cech, które chcemy wykorzystać do modelowania. Zakładamy, że kolumna\n",
    "\"Cecha_do_Prognozowania\" jest naszą zmienną docelową, a wszystkie inne kolumny numeryczne to nasze cechy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Wykorzystamy algorytm Lasów Losowych do określenia ważności poszczególnych cech w naszym zbiorze danych.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data.drop('Age', axis=1)\n",
    "target = data['Age']\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Tworzymy model Lasu Losowego\n",
    "forest = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Zakładając, że features_scaled i target zostały wcześniej zdefiniowane\n",
    "forest.fit(features_scaled, target)\n",
    "\n",
    "# Pobieramy ważności cech\n",
    "importances = forest.feature_importances_\n",
    "\n",
    "# Tworzymy DataFrame dla lepszej wizualizacji\n",
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': features.columns,  # Zakładając, że 'features' jest DataFrame\n",
    "    'Importance': importances\n",
    "})\n",
    "\n",
    "# Sortujemy cechy według ważności\n",
    "sorted_features = feature_importances.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Wizualizacja\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=sorted_features)\n",
    "plt.title('Ważność cech według Lasów Losowych')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na podstawie powyższej wizualizacji możemy wybrać cechy o największej ważności dla naszego problemu.\n",
    "\n",
    "### Wybór cech przy użyciu metody SelectKBest\n",
    "\n",
    "Używamy testu statystycznego f_classif do oceny cech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Zakładając, że chcemy wybrać 10 najlepszych cech\n",
    "selector = SelectKBest(score_func=f_classif, k=10)\n",
    "\n",
    "# Używamy selektora do oceny cech (z wyeliminowaniem 'ID')\n",
    "features_without_id = features.drop('ID', axis=1)\n",
    "best_features = selector.fit_transform(features_without_id, target)\n",
    "\n",
    "# Tworzymy DataFrame dla lepszej wizualizacji\n",
    "feature_scores = pd.DataFrame({\n",
    "    'Feature': features_without_id.columns, \n",
    "    'Score': selector.scores_\n",
    "})\n",
    "\n",
    "# Sortujemy cechy według wyniku\n",
    "sorted_scores = feature_scores.sort_values(by='Score', ascending=False)\n",
    "\n",
    "# Wizualizacja\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Score', y='Feature', data=sorted_scores)\n",
    "plt.title('Wyniki cech według metody SelectKBest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na podstawie powyższej wizualizacji możemy wybrać cechy o najwyższym wyniku, co wskazuje na ich potencjalne znaczenie dla naszego problemu klasyfikacji.\n",
    "\n",
    "### Rekursywna eliminacja cech (RFE)\n",
    "\n",
    "Używamy klasyfikatora SVM jako estymatora do oceny ważności cech i rekursywnego ich eliminowania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Zakładając, że chcemy wybrać 10 najlepszych cech\n",
    "estimator = SVC(kernel=\"linear\")\n",
    "selector = RFE(estimator, n_features_to_select=10, step=1)\n",
    "\n",
    "# Używamy RFE do oceny cech (z wyeliminowaniem 'ID')\n",
    "features_without_id = features.drop('ID', axis=1)\n",
    "selector = selector.fit(features_without_id, target)\n",
    "\n",
    "# Tworzenie DataFrame dla lepszej wizualizacji\n",
    "feature_ranking = pd.DataFrame({\n",
    "    'Feature': features_without_id.columns, \n",
    "    'Ranking': selector.ranking_\n",
    "})\n",
    "\n",
    "# Sortujemy cechy według rankingu\n",
    "sorted_ranking = feature_ranking.sort_values(by='Ranking')\n",
    "\n",
    "# Wizualizacja\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Ranking', y='Feature', data=sorted_ranking)\n",
    "plt.title('Ranking cech według metody RFE')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na podstawie powyższej wizualizacji możemy wybrać cechy o najniższym rankingu, co wskazuje na ich potencjalne znaczenie dla naszego problemu klasyfikacji. Im niższa wartość rankingu, tym ważniejsza jest cecha.\n",
    "\n",
    "### Połączenie trzech metod selekcji cech\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metoda Lasów Losowych\n",
    "forest = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "forest.fit(features_without_id, target)\n",
    "importances_rf = forest.feature_importances_\n",
    "\n",
    "# Metoda SelectKBest\n",
    "selector_kbest = SelectKBest(score_func=f_classif, k=features_without_id.shape[1]) # wybieramy wszystkie cechy\n",
    "best_features_kbest = selector_kbest.fit(features_without_id, target)\n",
    "scores_kbest = selector_kbest.scores_\n",
    "\n",
    "# Metoda RFE\n",
    "estimator = SVC(kernel=\"linear\")\n",
    "selector_rfe = RFE(estimator, n_features_to_select=1) # ranking wszystkich cech\n",
    "selector_rfe = selector_rfe.fit(features_without_id, target)\n",
    "ranking_rfe = selector_rfe.ranking_\n",
    "\n",
    "# Połączenie wyników\n",
    "combined_scores = pd.DataFrame({\n",
    "    'Feature': features_without_id.columns,\n",
    "    'Importance_RF': importances_rf,\n",
    "    'Score_KBest': scores_kbest,\n",
    "    'Ranking_RFE': max(ranking_rfe) - ranking_rfe + 1, # odwracamy ranking, aby większa wartość oznaczała ważniejszą cechę\n",
    "})\n",
    "\n",
    "combined_scores['Combined_Score'] = combined_scores['Importance_RF'] + combined_scores['Score_KBest'] + combined_scores['Ranking_RFE']\n",
    "\n",
    "# Sortujemy według skumulowanego wyniku\n",
    "sorted_combined_scores = combined_scores.sort_values(by='Combined_Score', ascending=False)\n",
    "\n",
    "# Wizualizacja\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Combined_Score', y='Feature', data=sorted_combined_scores)\n",
    "plt.title('Najważniejsze cechy według połączonych wyników trzech metod')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przekształcenia danych\n",
    "\n",
    "Możemy również przekształcić nasze dane, na przykład przez normalizację. Wiele modeli działa lepiej,\n",
    "gdy dane są znormalizowane. Poniżej znajduje się przykładowy kod, który wykonuje normalizację\n",
    "za pomocą metody Min-Max."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "features_scaled = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import niezbędnych bibliotek i podział danych\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Podziel dane na cechy i etykiety (target)\n",
    "X = data.drop('Absenteeism time in hours', axis=1)\n",
    "y = data['Absenteeism time in hours']\n",
    "\n",
    "# Podziel dane na zbiory treningowe i testowe\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Przekształcenie wybranych cech\n",
    "# Wybierz wybrane cechy\n",
    "selected_features = ['Weight', 'Service time', 'Distance from Residence to Work', 'Social drinker', 'Transportation expense']\n",
    "\n",
    "# Przekształć dane tylko dla wybranych cech\n",
    "scaler = StandardScaler()\n",
    "X_train_selected = scaler.fit_transform(X_train[selected_features])\n",
    "X_test_selected = scaler.transform(X_test[selected_features])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dyskretyzacja danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dyskretyzacja kolumny \"Distance from Residence to Work\"\n",
    "bins = [0, 10, 30, 50]  # zakresy\n",
    "labels = ['blisko', 'średnio daleko', 'daleko']\n",
    "X_train['Distance from Residence to Work'] = pd.cut(X_train['Distance from Residence to Work'], bins=bins, labels=labels, right=False)\n",
    "X_test['Distance from Residence to Work'] = pd.cut(X_test['Distance from Residence to Work'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Wyświetl pierwsze 5 wierszy, aby zobaczyć efekt dyskretyzacji\n",
    "X_train[['Distance from Residence to Work']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
